{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission to Mars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyMongo to work with MongoDBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define database and collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape a webpage and create a BeautifulSoup object from the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "# browser = Browser(\"chrome\", **executable_path, headless = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 NASA Mars News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Retrieve the data/information on NASA's Mars website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve page with the requests module\n",
    "executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "browser = Browser('chrome', **executable_path, headless = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = \"https://mars.nasa.gov/news/\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup = BS(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # URL of page to be scraped\n",
    "# # url = \"https://mars.nasa.gov/news/\"\n",
    "# # browser.visit(url)\n",
    "\n",
    "# # Retrieve page with the requests module\n",
    "# response = requests.get(url)\n",
    "# executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "# browser = Browser('chrome', **executable_path, headless = False)\n",
    "\n",
    "# # Create BeautifulSoup object: parse with \"html\"\n",
    "# soup = BS(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Print the html code of the NASA's Mars website\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Scrape the [NASA Mars News Site](https://mars.nasa.gov/news/) and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.1 Collect the latest News Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the div tags with a class = content_title\n",
    "div_tags_class_content_title = soup.body.find_all(\"div\", class_=\"content_title\")\n",
    "\n",
    "print(len(div_tags_class_content_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through and print the all of the div tags with a class = content_title with it's index numberr\n",
    "index_no =  0\n",
    "\n",
    "for news_title in div_tags_class_content_title: \n",
    "    print(f\"[{index_no}] {news_title.text}\")\n",
    "    index_no = index_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the div tage for the latest News Title\n",
    "print(div_tags_class_content_title[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the latest News Title as a variable\n",
    "news_title = div_tags_class_content_title[1].text\n",
    "print(news_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.2 Collect the latest Paragraph Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the div tags with a class = article_teaser_body\n",
    "div_tags_class_article_teaser_body = soup.body.find_all(\"div\", class_=\"article_teaser_body\")\n",
    "\n",
    "print(len(div_tags_class_article_teaser_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop through and print the all of the div tags with a class = article_teaser_body with it's index numberr\n",
    "index_no =  0\n",
    "\n",
    "for news_teaser in div_tags_class_article_teaser_body: \n",
    "    print(f\"[{index_no}] {news_teaser.text}\")\n",
    "    index_no = index_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the div tag for the latest Paragraph Text\n",
    "print(div_tags_class_article_teaser_body[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save the latest Paragraph Text as a variable\n",
    "news_p = div_tags_class_article_teaser_body[0].text\n",
    "print(news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Retrieve the data/information on NASA's JPL website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify location of chromedriver and store it as a variable\n",
    "# driverPath = !which chromedriver\n",
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve page with the requests module\n",
    "executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "browser = Browser('chrome', **executable_path, headless = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup = BS(html, \"html.parser\")\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the html code of the JPL Mars website\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect all the h3 tags with a class = release_date\n",
    "h3_tags_class_release_date = soup.body.find_all(\"h3\", class_=\"release_date\")\n",
    "\n",
    "print(len(h3_tags_class_release_date))\n",
    "type(h3_tags_class_release_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Determine which image is the most recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine which image is the most recent\n",
    "index_no = 0\n",
    "\n",
    "for img in h3_tags_class_release_date: \n",
    "    print(f\"[{index_no}] {img.text}\")\n",
    "#     img_date.append(img.text)\n",
    "    index_no = index_no + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -------------------------------------------------- OR --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through and print the all of the div tags with a class = content_title with it's index numberr\n",
    "index_no =  0\n",
    "img_date_list = []\n",
    "\n",
    "# Append the text of the div tags with a class = img to a list\n",
    "for img in h3_tags_class_release_date: \n",
    "#     print(f\"[{index_no}] {img.text}\")\n",
    "    img_date_list.append(img.text)\n",
    "    index_no = index_no + 1\n",
    "print(img_date)\n",
    "    \n",
    "# Convert to a Dataframe\n",
    "img_date_df = pd.DataFrame(img_date)\n",
    "img_date_df = img_date_df.rename(columns = {0: \"jpl_mars_image_date\"})\n",
    "print(img_date_df.dtypes)\n",
    "img_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the string objects to a datetime data type\n",
    "print(img_date_df.dtypes)\n",
    "date = img_date_df[\"jpl_mars_image_date\"][0]\n",
    "print(date)\n",
    "# new_date = datetime.datetime.strptime(date, '%B %d, %Y')\n",
    "# print(new_date)\n",
    "\n",
    "row_no = 0\n",
    "\n",
    "for date in img_date_df[\"jpl_mars_image_date\"]: \n",
    "    img_date_df[\"jpl_mars_image_date\"][row_no] = datetime.datetime.strptime(date, '%B %d, %Y')\n",
    "    row_no = row_no + 1\n",
    "#     print(date)\n",
    "#     print(f\"[{index_no}] {img.text}\")\n",
    "#     img_date.append(img.text)\n",
    "\n",
    "print(img_date_df.dtypes)\n",
    "img_date_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_no = 0\n",
    "# latest_date = img_date_df[\"jpl_mars_image_date\"][31]\n",
    "start_date_str = \"January 01, 0001\"\n",
    "start_date_obj = datetime.datetime.strptime(test_date_str , '%B %d, %Y')\n",
    "# print(latest_date)\n",
    "\n",
    "for date in img_date_df[\"jpl_mars_image_date\"]: \n",
    "    if date > start_date_obj:\n",
    "        start_date_obj = date\n",
    "#         print(start_date_obj)\n",
    "        \n",
    "# Get the index number and store it as a variable to get the image\n",
    "# https://stackoverflow.com/questions/31593201/how-are-iloc-and-loc-different\n",
    "\n",
    "# This returns a numpy.ndarray\n",
    "index_array = img_date_df[img_date_df[\"jpl_mars_image_date\"] == start_date_obj].index.values\n",
    "type(index_array)\n",
    "\n",
    "index = index_array[0]\n",
    "# index_str = index.astype(str)\n",
    "# index_str.replace(\"[\", \"\")\n",
    "print(index)\n",
    "\n",
    "# latest_date_index = img_date_df.loc[img_date_df[\"jpl_mars_image_date\"] == start_date_obj]\n",
    "# latest_date_index\n",
    "#     else:\n",
    "#         print(start_date_obj)\n",
    "        \n",
    "#     img_date_df[\"jpl_mars_image_date\"][row_no] = datetime.datetime.strptime(date, '%B %d, %Y')\n",
    "#     row_no = row_no + 1\n",
    "# #     print(date)\n",
    "#     print(f\"[{index_no}] {img.text}\")\n",
    "#     img_date.append(img.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_str = 'Jan 01 0001 12:00AM'\n",
    "date_time_obj = datetime.datetime.strptime(date_time_str, '%b %d %Y %I:%M%p')\n",
    "\n",
    "print('Date:', date_time_obj.date())\n",
    "print('Time:', date_time_obj.time())\n",
    "print('Date-time:', date_time_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n",
    "# df\n",
    "df = df.rename(columns={\"A\": \"a\", \"B\": \"c\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Collect the div tag for the current Featured Mars Image and its url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect all the div tags with a class = img\n",
    "div_tags_class_img = soup.body.find_all(\"a\", class_=\"button fancybox\")\n",
    "\n",
    "print(len(div_tags_class_img))\n",
    "type(div_tags_class_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use splinter to click through the website\n",
    "# https://splinter.readthedocs.io/en/latest/api/driver-and-element-api.html\n",
    "\n",
    "# Click FULL IMAGE to see a large thumbnail of the featured image \n",
    "browser.click_link_by_partial_text('FULL IMAGE')\n",
    "\n",
    "# # Click more info to go to the webpage for the featured image\n",
    "time.sleep(2)\n",
    "browser.click_link_by_partial_text('more info')\n",
    "\n",
    "# # Click the featured image for the large image\n",
    "time.sleep(2)\n",
    "browser.click_link_by_partial_href('.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for all the img tags\n",
    "large_img = browser.find_by_tag(\"img\")\n",
    "print(type(large_img))\n",
    "\n",
    "# Save the img tag to the featured_img_url variable\n",
    "for img in large_img:\n",
    "    featured_img_url = img._element.get_attribute('src')\n",
    "    print(featured_img_url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to scrape the table containing facts about Mars\n",
    "mars_facts_tables = pd.read_html('https://space-facts.com/mars/')\n",
    "print(f\"Total Number of Tabls on the Site: {len(mars_facts_tables)}\")\n",
    "print(\"************************************************************\")\n",
    "\n",
    "# Use a loop to determine which is the preferred table\n",
    "for table in mars_facts_tables:\n",
    "    print(table)\n",
    "    print(\"************************************************************\")\n",
    "# Mars_Facts_Table[0]\n",
    "\n",
    "# # Take second table for Mars facts\n",
    "# Mars_Facts_Table_df = Mars_Facts_Table[0]\n",
    "# Mars_Facts_Table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types\n",
    "print(type(mars_facts_tables))\n",
    "print(type(mars_facts_tables[0]))\n",
    "\n",
    "# Save the desired table, which is a dataframe, to a variable\n",
    "mars_facts_table_df = mars_facts_tables[0]\n",
    "print(type(mars_facts_table_df))\n",
    "mars_facts_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the Dataframe Table back into html\n",
    "mars_facts_table_html = mars_facts_table_df.to_html()\n",
    "mars_facts_table_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/chromedriver\r\n"
     ]
    }
   ],
   "source": [
    "# identify location of chromedriver and store it as a variable\n",
    "# driverPath = !which chromedriver\n",
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve page with the requests module\n",
    "executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "browser = Browser('chrome', **executable_path, headless = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "soup = BS(html, \"html.parser\")\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the html code of the JPL Mars website\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use splinter to click through the website\n",
    "# https://stackoverflow.com/questions/45364497/cannot-chain-find-and-find-all-in-beautifulsoup\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/#attributes\n",
    "\n",
    "# Click FULL IMAGE to see a large thumbnail of the featured image \n",
    "mars_hemispheres = soup.find_all(\"div\", class_ = \"item\")\n",
    "# mars_hemispheres.find(\"a\")\n",
    "print(type(mars_hemispheres))\n",
    "print(f\"Number of <div> tags: {len(mars_hemispheres)}\")\n",
    "# print(mars_hemispheres)\n",
    "# print(\"************************************************************\")\n",
    "# print(\"************************************************************\")\n",
    "# print(type(mars_hemispheres[0]))\n",
    "# print(f\"Number of <a> tags: {len(mars_hemispheres[0].find_all('a'))}\")\n",
    "# mars_hemispheres_a_tags = mars_hemispheres[0].find_all('a')\n",
    "# print(mars_hemispheres_a_tags)\n",
    "# print(\"************************************************************\")\n",
    "# print(\"************************************************************\")\n",
    "# print(type(mars_hemispheres_a_tags[0]))\n",
    "# print(f\"Number of : {len(mars_hemispheres_a_tags[0].find_all('img'))}\")\n",
    "# mars_hemispheres_a_tags_img_tag = mars_hemispheres_a_tags[0].find_all('img', class_ = \"thumb\")\n",
    "# print(mars_hemispheres_a_tags_img_tag[0])\n",
    "# # print(mars_hemispheres_a_tags_img_tag._element.get_attribute('src'))\n",
    "# print(\"************************************************************\")\n",
    "# print(\"************************************************************\")\n",
    "# print(type(mars_hemispheres_a_tags_img_tag[0]))\n",
    "# mars_hemispheres_a_tags_img_tag = mars_hemispheres_a_tags_img_tag[0]\n",
    "# print(mars_hemispheres_a_tags_img_tag[\"src\"])\n",
    "# print(\"************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_hemispheres_url = \"https://astrogeology.usgs.gov/\"\n",
    "\n",
    "for div_tag in mars_hemispheres:\n",
    "    mars_hemispheres_div_a_tags = div_tag.find_all(\"a\")\n",
    "#     print(mars_hemispheres_div_a_tags)\n",
    "    \n",
    "    for a_tag in mars_hemispheres_div_a_tags:\n",
    "        mars_hemispheres_div_a_img_tags = a_tag.find_all(\"img\", class_ = \"thumb\")\n",
    "#         print(mars_hemispheres_div_a_img_tags)\n",
    "        \n",
    "        for attr_src in mars_hemispheres_div_a_img_tags:\n",
    "            mars_hemispheres_div_a_img_tags_attr_src = attr_src[\"src\"]\n",
    "            print(mars_hemispheres_url + mars_hemispheres_div_a_img_tags_attr_src)\n",
    "# https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerberus Hemisphere Enhanced\n",
      "https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg\n",
      "Schiaparelli Hemisphere Enhanced\n",
      "https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg\n",
      "Syrtis Major Hemisphere Enhanced\n",
      "https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg\n",
      "Valles Marineris Hemisphere Enhanced\n",
      "https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg\n"
     ]
    }
   ],
   "source": [
    "# References\n",
    "# - Find And Replace | Python String:\n",
    "#     - https://www.geeksforgeeks.org/python-string-replace/\n",
    "# - Ignore Future Warning Message:     \n",
    "#     - https://stackoverflow.com/questions/57704412/how-to-suppress-future-warning-tensorflow\n",
    "# - Splinter Broswer Documentation \n",
    "#     - https://splinter.readthedocs.io/en/latest/browser.html#managing-windows\n",
    "# - Splinter Finding Documentation \n",
    "#     - https://splinter.readthedocs.io/en/latest/finding.html\n",
    "# - Dictionaries:     \n",
    "#     - https://www.geeksforgeeks.org/add-a-keyvalue-pair-to-dictionary-in-python/\n",
    "#     - https://www.kite.com/python/answers/how-to-append-a-value-to-an-empty-dictionary-in-python\n",
    "\n",
    "\n",
    "\n",
    "# # Save the img tag to the featured_img_url variable\n",
    "# mars_hemispheres = soup.find_all(\"div\", class_ = \"item\")\n",
    "# print(len(mars_hemispheres))\n",
    "# print(mars_hemispheres[0])\n",
    "# print(\"************************************************************\")\n",
    "# print(\"************************************************************\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=r\"browse\", category=FutureWarning)\n",
    "\n",
    "hemisphere_image_urls_list = []\n",
    "\n",
    "h3_mars_hemispheres = soup.find_all(\"h3\")\n",
    "for hemisphere in range(len(h3_mars_hemispheres)):\n",
    "#     print(len(h3_mars_hemispheres))\n",
    "    mars_hemisphere_name = h3_mars_hemispheres[hemisphere].string.replace(\"<h3>\", \"\")\n",
    "    print(mars_hemisphere_name)\n",
    "    \n",
    "    \n",
    "    browser.windows.current = browser.windows[0]\n",
    "#     time.sleep(1)\n",
    "    browser.click_link_by_partial_text(mars_hemisphere_name)\n",
    "#     time.sleep(1)\n",
    "    browser.click_link_by_text(\"Sample\")\n",
    "    browser.windows.current = browser.windows[1]\n",
    "    mars_hemisphere_url = browser.find_by_tag(\"img\")[\"src\"]\n",
    "    print(mars_hemisphere_url)\n",
    "    \n",
    "    hemisphere_title_and_image_url_dict = {\"title\": mars_hemisphere_name, \"img_url\": mars_hemisphere_url}\n",
    "    hemisphere_image_urls_list.append(hemisphere_title_and_image_url_dict)\n",
    "\n",
    "#     time.sleep(1)\n",
    "    browser.windows[1].close()\n",
    "    browser.back()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/cerberus_enhanced.tif/full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/schiaparelli_enhanced.tif/full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/syrtis_major_enhanced.tif/full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/valles_marineris_enhanced.tif/full.jpg'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hemisphere_image_urls_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the img tag to the featured_img_url variable\n",
    "# mars_hemispheres = soup.find_all(\"div\", class_ = \"item\")\n",
    "# print(len(mars_hemispheres))\n",
    "# print(mars_hemispheres[0])\n",
    "# print(\"************************************************************\")\n",
    "# print(\"************************************************************\")\n",
    "h3_mars_hemispheres = soup.find_all(\"h3\")\n",
    "for hemisphere in range(len(h3_mars_hemispheres)):\n",
    "#     print(len(h3_mars_hemispheres))\n",
    "    print(h3_mars_hemispheres[hemisphere].string.replace(\"<h3>\", \"\"))\n",
    "    mars_hemisphere_name = h3_mars_hemispheres[hemisphere].string.replace(\"<h3>\", \"\")\n",
    "        # for div_tag in mars_hemispheres:\n",
    "        #     mars_hemispheres_div_a_tags = div_tag.find_all(\"a\", class_ = \"itemLink product-item\")\n",
    "        #     window.close()\n",
    "        #     window.close()\n",
    "\n",
    "\n",
    "\n",
    "# for hemisphere in range(1):\n",
    "\n",
    "    browser.windows.current = browser.windows[0]\n",
    "    time.sleep(4)\n",
    "    browser.click_link_by_partial_text(mars_hemisphere_name)\n",
    "    time.sleep(4)\n",
    "    browser.click_link_by_text(\"Sample\")\n",
    "    browser.windows.current = browser.windows[1]\n",
    "    print(browser.find_by_tag(\"img\")[\"src\"])\n",
    "#     window.is_current = True\n",
    "#     window = browser.windows[0]\n",
    "#     browser.windows.current = browser.windows[hemisphere]\n",
    "    time.sleep(4)\n",
    "    browser.windows[1].close()\n",
    "    browser.back()\n",
    "#         browser.back()\n",
    "#         time.sleep(4)\n",
    "    #     window.close()\n",
    "    #     window.next\n",
    "    #     window.is_current = True\n",
    "\n",
    "#     print(len(browser.windows))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     browser.click_link_by_partial_href(\"https\")  \n",
    "# #     print(type(browser.click_link_by_partial_text(\"Sample\")))\n",
    "# #     print(browser.html)\n",
    "#     print(browser.find_by_tag(\"img\")[\"src\"])\n",
    "#     print(mars_hemispheres_div_a_tags)\n",
    "#     print(\"************************************************************\")\n",
    "#     print(\"************************************************************\")\n",
    "    \n",
    "#     for a_tag_I in mars_hemispheres_div_a_tags:\n",
    "#         mars_hemispheres_div_a_h3_tags = a_tag_I.find_by_text(\"Enhanced\")\n",
    "#         print(mars_hemispheres_div_a_h3_tags)\n",
    "# #         time.sleep(2)\n",
    "\n",
    "#         print(\"************************************************************\")\n",
    "#         print(\"************************************************************\")\n",
    "        \n",
    "#         for a_tag_II in mars_hemispheres_div_a_tags:\n",
    "#             mars_hemispheres_div_a_a_tags = a_tag_II.find_all(\"a\")\n",
    "#             print(mars_hemispheres_div_a_a_tags)\n",
    "    \n",
    "    \n",
    "#     time.sleep(2)\n",
    "#     browser.click_link_by_partial_text(hemisphere)\n",
    "#     featured_img_url = hemisphere._element.get_attribute('src')\n",
    "#     print(featured_img_url)\n",
    "\n",
    "# Click more info to go to the webpage for the featured image\n",
    "# time.sleep(2)\n",
    "# browser.click_link_by_partial_text(\"Open\")\n",
    "\n",
    "# # Click the featured image for the large image\n",
    "# time.sleep(2)\n",
    "# browser.click_link_by_partial_href('.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******************************************************************************************\n",
    "# I should use the below code for the Step 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect all the h3 tags with a class = release_date\n",
    "h3_tags_class_release_date = soup.body.find_all(\"h3\", class_=\"release_date\")\n",
    "\n",
    "print(len(h3_tags_class_release_date))\n",
    "type(h3_tags_class_release_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Determine which image is the most recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine which image is the most recent\n",
    "index_no = 0\n",
    "\n",
    "for img in h3_tags_class_release_date: \n",
    "    print(f\"[{index_no}] {img.text}\")\n",
    "#     img_date.append(img.text)\n",
    "    index_no = index_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect the div tags with a class = img\n",
    "div_tags_class_img[index]\n",
    "\n",
    "# # Determine which image is the most recent\n",
    "# index_no = 0\n",
    "\n",
    "# for img in h3_tags_class_release_date: \n",
    "#     print(f\"[{index_no}] {img.text}\")\n",
    "# #     img_date.append(img.text)\n",
    "#     index_no = index_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Click again for full large image\n",
    "time.sleep(2)\n",
    "browser.click_link_by_partial_text('more info')\n",
    "\n",
    "html = browser.html\n",
    "soup = BS(html, 'html.parser')\n",
    "\n",
    "# Search for image source\n",
    "results = soup.find_all('figure', class_='lede')\n",
    "relative_img_path = results[0].a['href']\n",
    "featured_img = 'https://www.jpl.nasa.gov' + relative_img_path\n",
    "\n",
    "print(featured_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect all the h3 tags with a class = release_date\n",
    "h3_tags_class_release_date = soup.body.find_all(\"h3\", class_=\"release_date\")\n",
    "\n",
    "print(len(h3_tags_class_release_date))\n",
    "type(h3_tags_class_release_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Determine which image is the most recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine which image is the most recent\n",
    "index_no = 0\n",
    "\n",
    "for img in h3_tags_class_release_date: \n",
    "    print(f\"[{index_no}] {img.text}\")\n",
    "#     img_date.append(img.text)\n",
    "    index_no = index_no + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END \n",
    "# I should use the below code for the Step 1.1\n",
    "******************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over multiple pages and scrape content from each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "html = browser.html\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BS(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = soup.find_all(\"div\", class_ = \"img\")\n",
    "len(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open browser to JPL Featured Image\n",
    "browser.visit('https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars')\n",
    "\n",
    "# Click through to find full image\n",
    "browser.click_link_by_partial_text('FULL IMAGE')\n",
    "\n",
    "# Click again for full large image\n",
    "time.sleep(3)\n",
    "browser.click_link_by_partial_text('more info')\n",
    "\n",
    "html = browser.html\n",
    "soup = BS(html, 'html.parser')\n",
    "\n",
    "# Search for image source\n",
    "results = soup.find_all('figure', class_='lede')\n",
    "relative_img_path = results[0].a['href']\n",
    "featured_img = 'https://www.jpl.nasa.gov' + relative_img_path\n",
    "\n",
    "print(featured_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define URL to scrape and inform the browser to visit the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # identify location of chromedriver and store it as a variable\n",
    "# driverPath = !which chromedriver\n",
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration variables to enable Splinter to interact with browser\n",
    "executable_path = {'executable_path': driverPath[0]}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the results, then input results containing desired info into MongoDB collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div_tags_class_content_title = soup.body.find_all(\"div\", class_=\"content_title\")\n",
    "# # a_tags = div_tags_class_content_title.find_all(\".content_title > a\")\n",
    "\n",
    "# current_titles = []\n",
    "# for title in div_tags_class_content_title:\n",
    "# #     current_titles = div_tags_class_content_title.find_by_tag(\"a\")\n",
    "#     current_titles.append(div_tags_class_content_title.find_elements_by_tag_name(\"a\"))\n",
    "\n",
    "# ?????? I am not sure why I can't loop through and print each title inside the div tags ??????\n",
    "# for title in div_tags_class_content_title:\n",
    "#     print_title = div_tags_class_content_title[title].text\n",
    "#     print(print_title)\n",
    "\n",
    "\n",
    "# # print(len(div_tags_class_content_title))\n",
    "# # print(div_tags_class_content_title[0])\n",
    "# print(a_tags[7])\n",
    "# # print(current_titles)\n",
    "\n",
    "# # a_tags_class_href = soup.body.find_all(\"a\")\n",
    "# # print(len(a_tags_class_href))\n",
    "# # print(a_tags_class_href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_tags_class_article_teaser_body = soup.body.find_all(\"div\", class_=\"article_teaser_body\")\n",
    "print(len(div_tags_class_article_teaser_body))\n",
    "# print(div_tags_class_article_teaser_body)\n",
    "print(type(div_tags_class_article_teaser_body))\n",
    "\n",
    "for test in div_tags_class_article_teaser_body:\n",
    "    print(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = soup.body.find_all(\"a\")\n",
    "print(len(a_tags))\n",
    "print(type(a_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for a in a_tags:\n",
    "    print(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_titles = soup.find_all(\"div\", class_ = \"content_title\")\n",
    "len(news_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for title in news_titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_titles[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h3_tags = soup.body.find_all(\"div\", id_=\"main_container\")\n",
    "# len(h3_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tags = soup.body.find_all(\"div\", class_ = \"rollover_description_inner\")\n",
    "len(description_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for description in description_tags:\n",
    "    print(description.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_tags[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"News Title: \\t {news_titles[0].text.strip()}\")\n",
    "print()\n",
    "print(f\"Description: \\t {description_tags[0].text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph_text = soup.find(\"div\", class_ = \"article_teaser_body\")\n",
    "# # len(Paragraph_text)\n",
    "# print(Paragraph_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# div_tags = soup.body.find_all(\"div\")\n",
    "# len(div_tags)\n",
    "# # print(div_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for div in div_tags:\n",
    "#     print(div.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the latest element that contains news title and news_paragraph\n",
    "news_title = soup.find('div', class_='content_title').find('a').text\n",
    "news_p = soup.find('div', class_='article_teaser_body')\n",
    "\n",
    "# Display scrapped data \n",
    "print(news_title)\n",
    "print(news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visit the USGS Astrogeology site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open browser to USGS Astrogeology site\n",
    "browser.visit('https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for thumbnail links\n",
    "thumbnail_results = results[0].find_all('a')\n",
    "thumbnail_links = []\n",
    "\n",
    "for thumbnail in thumbnail_results:\n",
    "    \n",
    "    # If the thumbnail element has an image...\n",
    "    if (thumbnail.img):\n",
    "        \n",
    "        # then grab the attached link\n",
    "        thumbnail_url = 'https://astrogeology.usgs.gov/' + thumbnail['href']\n",
    "        \n",
    "        # Append list with links\n",
    "        thumbnail_links.append(thumbnail_url)\n",
    "\n",
    "len(thumbnail_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnail_links[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
